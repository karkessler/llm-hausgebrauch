{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e428d3",
   "metadata": {},
   "source": [
    "# Next Token Prediction – live & spielerisch (Toy-Notebook)\n",
    "\n",
    "**Didaktisches Ziel:** *LLMs wählen das wahrscheinlichste nächste Token (bzw. sampeln daraus) – nicht „die Wahrheit“.*\n",
    "\n",
    "In diesem Notebook bauen wir eine **Mini-Version** der letzten Schritte eines LLMs nach:\n",
    "\n",
    "1. **Logits** (unskalierte Scores) für ein kleines Vokabular  \n",
    "2. **Softmax** → Wahrscheinlichkeiten  \n",
    "3. **Temperatur** (Temperature) steuert „Zufälligkeit“  \n",
    "4. **Argmax** vs. **Sampling** (Ziehen nach Wahrscheinlichkeit)  \n",
    "5. Mehrere Durchläufe → unterschiedliche Outputs\n",
    "\n",
    "> **Wichtig:** Das ist kein echtes LLM – aber es zeigt die **entscheidende Mechanik** am Ende der Pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ee63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Für reproduzierbare Zufallszahlen (Sampling)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a3484",
   "metadata": {},
   "source": [
    "## 1) Mini-Vokabular & Beispiel-Logits\n",
    "\n",
    "Wir nehmen ein **Mini-Vokabular** (5–10 Tokens) und tun so, als hätte das Modell für den nächsten Token bereits Logits berechnet.\n",
    "\n",
    "- **Logits** sind einfach reelle Zahlen (Scores).  \n",
    "- Höherer Logit → tendenziell höhere Wahrscheinlichkeit nach Softmax.\n",
    "\n",
    "> In echten Modellen kommen diese Logits aus einer Projektion auf den Vokabularraum (z. B. `z = W_out · h + b`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Vokabular (Tokens)\n",
    "vocab = [\"Paris\", \"Berlin\", \"Rom\", \"Frankreich\", \".\", \"<EOS>\"]\n",
    "\n",
    "# Beispiel: Logits für \"nächstes Token\"\n",
    "# (Diese Zahlen sind frei gewählt – wir wollen nur die Mechanik sehen.)\n",
    "logits = np.array([2.4, 1.2, 0.3, 2.0, 0.1, -0.5], dtype=float)\n",
    "\n",
    "print(\"Vokabular:\", vocab)\n",
    "print(\"Logits:   \", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa70ea3",
   "metadata": {},
   "source": [
    "## 2) Softmax (numerisch stabil)\n",
    "\n",
    "Softmax macht aus Logits Wahrscheinlichkeiten:\n",
    "\n",
    "\\[\n",
    "p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "\\]\n",
    "\n",
    "**Numerische Stabilität:** Wir subtrahieren das Maximum, damit `exp()` nicht überläuft.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Numerisch stabile Softmax.\"\"\"\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    z_shift = z - np.max(z)          # Stabilitätstrick\n",
    "    exp_z = np.exp(z_shift)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "probs = softmax(logits)\n",
    "print(\"Wahrscheinlichkeiten:\", probs)\n",
    "print(\"Summe:\", probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d7d0f",
   "metadata": {},
   "source": [
    "### Visualisierung der Wahrscheinlichkeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probs(tokens, p, title=\"Wahrscheinlichkeiten (Softmax)\"):\n",
    "    x = np.arange(len(tokens))\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.bar(x, p)\n",
    "    plt.xticks(x, tokens, rotation=20, ha=\"right\")\n",
    "    plt.ylim(0, max(0.01, float(np.max(p)) * 1.15))\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"p\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_probs(vocab, probs, title=\"Softmax auf Logits (T = 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5251c4",
   "metadata": {},
   "source": [
    "## 3) Temperatur (Temperature)\n",
    "\n",
    "Temperatur skaliert Logits **vor** der Softmax:\n",
    "\n",
    "\\[\n",
    "p_i(T) = \\mathrm{softmax}\\left(\\frac{z_i}{T}\\right)\n",
    "\\]\n",
    "\n",
    "- **T < 1** → Verteilung wird *spitzer* (mehr „deterministisch“)  \n",
    "- **T = 1** → unverändert  \n",
    "- **T > 1** → Verteilung wird *flacher* (mehr Variation)\n",
    "\n",
    "> Viele Systeme nutzen zusätzlich Top-k / Top-p (Nucleus Sampling). Hier bleiben wir bewusst beim Kern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(z: np.ndarray, T: float) -> np.ndarray:\n",
    "    if T <= 0:\n",
    "        raise ValueError(\"Temperatur T muss > 0 sein.\")\n",
    "    return softmax(z / T)\n",
    "\n",
    "for T in [0.5, 1.0, 1.5, 2.0]:\n",
    "    pT = softmax_with_temperature(logits, T)\n",
    "    entropy = float(-(pT * np.log(pT + 1e-12)).sum())\n",
    "    print(f\"T={T:>3}: p_max={pT.max():.4f}, Entropie={entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in [0.5, 1.0, 2.0]:\n",
    "    plot_probs(vocab, softmax_with_temperature(logits, T), title=f\"Softmax mit Temperatur T={T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484ceb7",
   "metadata": {},
   "source": [
    "## 4) Argmax vs. Sampling\n",
    "\n",
    "**Argmax** wählt immer den Token mit maximaler Wahrscheinlichkeit:\n",
    "\\[ \\hat{i} = \\arg\\max_i p_i \\]\n",
    "\n",
    "**Sampling** zieht zufällig gemäß der Wahrscheinlichkeitsverteilung:\n",
    "- Bei gleicher Verteilung kann in verschiedenen Durchläufen **Verschiedenes** herauskommen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd098224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_choice(tokens, p):\n",
    "    idx = int(np.argmax(p))\n",
    "    return tokens[idx], idx\n",
    "\n",
    "def sample_choice(tokens, p, rng: np.random.Generator):\n",
    "    idx = int(rng.choice(len(tokens), p=p))\n",
    "    return tokens[idx], idx\n",
    "\n",
    "p = softmax_with_temperature(logits, T=1.0)\n",
    "\n",
    "am_token, am_idx = argmax_choice(vocab, p)\n",
    "print(\"Argmax:\", am_token, \"(idx=\", am_idx, \")\")\n",
    "\n",
    "samples = [sample_choice(vocab, p, rng)[0] for _ in range(15)]\n",
    "print(\"Sampling (15x):\", samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a553216",
   "metadata": {},
   "source": [
    "## 5) Mehrere Durchläufe: Wie oft kommt welcher Token?\n",
    "\n",
    "Wir simulieren viele Samples und zählen Häufigkeiten.  \n",
    "Damit sieht man: **Seltene Tokens kommen seltener, aber nicht nie.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_many(tokens, p, n=2000, rng=None):\n",
    "    rng = rng or np.random.default_rng()\n",
    "    idxs = rng.choice(len(tokens), size=n, p=p)\n",
    "    counts = np.bincount(idxs, minlength=len(tokens))\n",
    "    rel = counts / n\n",
    "    return counts, rel\n",
    "\n",
    "T = 1.2\n",
    "pT = softmax_with_temperature(logits, T)\n",
    "counts, rel = sample_many(vocab, pT, n=3000, rng=rng)\n",
    "\n",
    "print(\"Temperatur:\", T)\n",
    "print(\"Theorie p:\", pT)\n",
    "print(\"Empirie :\", rel)\n",
    "\n",
    "plot_probs(vocab, rel, title=f\"Empirische Häufigkeiten (Sampling, n=3000, T={T})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68ffc0",
   "metadata": {},
   "source": [
    "## 6) Mini-Experiment: „Wahr“ vs. „Wahrscheinlich“\n",
    "\n",
    "Stell dir vor, du willst, dass nach *„Die Hauptstadt von Frankreich ist“* das Token **„Paris“** kommt.\n",
    "Wenn die Logits aber so aussehen, dass **„Frankreich“** oder **„.“** auch relativ wahrscheinlich sind,\n",
    "kann Sampling (je nach Temperatur) gelegentlich „falsch“ wirkende Tokens erzeugen.\n",
    "\n",
    "**Kernpunkt:** Ein LLM ist kein Wahrheitsautomat, sondern eine Wahrscheinlichkeitsmaschine im Kontext.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2 = logits.copy()\n",
    "logits2[vocab.index(\"Paris\")] = 1.6\n",
    "logits2[vocab.index(\"Frankreich\")] = 2.2\n",
    "logits2[vocab.index(\".\")] = 1.4\n",
    "\n",
    "for T in [0.7, 1.0, 1.3]:\n",
    "    p2 = softmax_with_temperature(logits2, T)\n",
    "    token_am, _ = argmax_choice(vocab, p2)\n",
    "    samples2 = [sample_choice(vocab, p2, rng)[0] for _ in range(12)]\n",
    "    print(f\"\\nT={T}\")\n",
    "    print(\"Argmax:\", token_am)\n",
    "    print(\"Sampling:\", samples2)\n",
    "    plot_probs(vocab, p2, title=f\"Variante logits2 – Softmax mit T={T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b194f8",
   "metadata": {},
   "source": [
    "## 7) Aufgaben (für Studierende)\n",
    "\n",
    "1. **Temperatur-Experiment:** Probiere `T=0.2`, `T=3.0`. Was passiert?  \n",
    "2. **Logits selbst wählen:** Setze `logits` so, dass zwei Tokens fast gleich wahrscheinlich sind.  \n",
    "3. **Sampling-Frequenz:** Erhöhe `n` auf 10000. Wie nah ist die Empirie an der Theorie?  \n",
    "4. (Optional) Implementiere **Top-k Sampling**: setze alle Wahrscheinlichkeiten außer den k größten auf 0 und normiere neu.\n",
    "\n",
    "---\n",
    "\n",
    "### Take-away (Merksatz)\n",
    "**Ein LLM wählt Tokens nach Wahrscheinlichkeiten – nicht nach „Wahrheit“.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
