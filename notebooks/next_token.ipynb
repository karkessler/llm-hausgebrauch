{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Token Vorhersage – Rechenbeispiel (wie auf der Folie)\n",
        "\n",
        "Wir verwenden den **Kontextvektor nach Attention** (z. B. für das aktuelle Token „von“):\n",
        "\n",
        "\\[\n",
        "h = [0.304,\\ 0.529,\\ 0.603,\\ 0.399]\n",
        "\\]\n",
        "\n",
        "Mini-Vokabular:\n",
        "\\[\n",
        "V = \\{\\text{Frankreich},\\ \\text{Deutschland},\\ \\text{Spanien}\\}\n",
        "\\]\n",
        "\n",
        "Lineare Projektion (vereinfacht):\n",
        "\\[\n",
        "z = W_{out}\\,h + b\n",
        "\\]\n",
        "\n",
        "Dann Softmax:\n",
        "\\[\n",
        "p(token) = \\frac{e^{z_{token}}}{\\sum_k e^{z_k}}\n",
        "\\]\n",
        "\n",
        "Zusätzlich enthält dieses Notebook:\n",
        "- **Schritt-für-Schritt-Ausgabe** (für Live-Demo)\n",
        "- **Balkendiagramm** der Softmax-Wahrscheinlichkeiten\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 0: Voraussetzungen (h + Mini-Vokabular + W_out)\n",
        "\n",
        "Gewichtungsvektoren (Zeilen in \\(W_{out}\\)) wie auf der Folie:\n",
        "\n",
        "- Frankreich: \\([1.0,\\ 0.5,\\ 1.0,\\ 0.5]\\)\n",
        "- Deutschland: \\([0.0,\\ 1.0,\\ 0.0,\\ 1.0]\\)\n",
        "- Spanien: \\([1.0,\\ 0.0,\\ 1.0,\\ 0.0]\\)\n",
        "\n",
        "Bias \\(b\\) setzen wir hier der Einfachheit halber auf 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h = np.array([0.304, 0.529, 0.603, 0.399], dtype=float)\n",
        "\n",
        "vocab = [\"Frankreich\", \"Deutschland\", \"Spanien\"]\n",
        "\n",
        "W_out = {\n",
        "    \"Frankreich\":  np.array([1.0, 0.5, 1.0, 0.5], dtype=float),\n",
        "    \"Deutschland\": np.array([0.0, 1.0, 0.0, 1.0], dtype=float),\n",
        "    \"Spanien\":     np.array([1.0, 0.0, 1.0, 0.0], dtype=float)\n",
        "}\n",
        "\n",
        "b = {t: 0.0 for t in vocab}\n",
        "\n",
        "df_W = pd.DataFrame({t: W_out[t] for t in vocab}).T\n",
        "df_W.columns = [\"w1\", \"w2\", \"w3\", \"w4\"]\n",
        "df_W\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 1: Logits berechnen\n",
        "\n",
        "Für jedes Token:\n",
        "\\[\n",
        "z_{token} = W_{out,token} \\cdot h + b_{token}\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = {}\n",
        "for t in vocab:\n",
        "    logits[t] = float(W_out[t] @ h + b[t])\n",
        "\n",
        "df_logits = pd.DataFrame({\"Logit z\": [logits[t] for t in vocab]}, index=vocab)\n",
        "df_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 2: Exponentialwerte \\(e^z\\) und Summe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp_vals = {t: float(np.exp(logits[t])) for t in vocab}\n",
        "sum_exp = float(sum(exp_vals.values()))\n",
        "\n",
        "df_exp = pd.DataFrame({\n",
        "    \"Logit z\": [logits[t] for t in vocab],\n",
        "    \"e^z\": [exp_vals[t] for t in vocab]\n",
        "}, index=vocab)\n",
        "\n",
        "df_exp.loc[\"Summe\", \"e^z\"] = sum_exp\n",
        "df_exp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schritt 3: Softmax-Wahrscheinlichkeiten\n",
        "\n",
        "\\[\n",
        "p(token) = \\frac{e^{z_{token}}}{\\sum_k e^{z_k}}\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probs = {t: exp_vals[t] / sum_exp for t in vocab}\n",
        "\n",
        "df_probs = pd.DataFrame({\n",
        "    \"Logit z\": [logits[t] for t in vocab],\n",
        "    \"Softmax p\": [probs[t] for t in vocab]\n",
        "}, index=vocab)\n",
        "df_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ergebnis: nächstes Token (argmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "next_token = max(probs, key=probs.get)\n",
        "print(\"Nächstes Token (höchste Wahrscheinlichkeit):\", next_token)\n",
        "print(\"Wahrscheinlichkeit:\", round(probs[next_token], 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zusatz A: Schritt-für-Schritt-Ausgabe (für Live-Demo)\n",
        "\n",
        "Diese Zelle gibt jede Zwischenrechnung so aus, dass du sie direkt vorlesen kannst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fmt(v, nd=3):\n",
        "    return \"[\" + \", \".join(f\"{x:.{nd}f}\" for x in v) + \"]\"\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"NEXT TOKEN VORHERSAGE – Schritt für Schritt\")\n",
        "print(\"=\"*72)\n",
        "print(\"Gegeben: h =\", fmt(h, 3))\n",
        "print(\"Vokabular:\", vocab)\n",
        "\n",
        "print(\"\\nSchritt 1: Logits z = W_out · h + b (hier b=0)\")\n",
        "for t in vocab:\n",
        "    w = W_out[t]\n",
        "    terms = w * h\n",
        "    z = logits[t]\n",
        "    print(f\"\\nToken: {t}\")\n",
        "    print(\"  w       =\", fmt(w, 3))\n",
        "    print(\"  w ⊙ h   =\", fmt(terms, 3), \"(elementweise Produkte)\")\n",
        "    print(\"  z       = sum(w_i*h_i) =\", f\"{z:.3f}\")\n",
        "\n",
        "print(\"\\nSchritt 2: Exponentialwerte\")\n",
        "for t in vocab:\n",
        "    print(f\"  e^{logits[t]:.3f} = {exp_vals[t]:.3f}\")\n",
        "print(\"  Summe =\", f\"{sum_exp:.3f}\")\n",
        "\n",
        "print(\"\\nSchritt 3: Softmax\")\n",
        "for t in vocab:\n",
        "    print(f\"  p({t}) = {exp_vals[t]:.3f}/{sum_exp:.3f} = {probs[t]:.3f}\")\n",
        "\n",
        "print(\"\\nErgebnis:\")\n",
        "print(\"  nächstes Token =\", next_token)\n",
        "print(\"  p              =\", f\"{probs[next_token]:.3f}\")\n",
        "print(\"=\"*72)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zusatz B: Mini-Visualisierung (Softmax-Wahrscheinlichkeiten)\n",
        "\n",
        "Balkendiagramm der Wahrscheinlichkeiten für die Kandidaten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = vocab\n",
        "values = np.array([probs[t] for t in vocab], dtype=float)\n",
        "\n",
        "order = np.argsort(values)[::-1]\n",
        "labels_sorted = [labels[i] for i in order]\n",
        "values_sorted = values[order]\n",
        "\n",
        "plt.figure(figsize=(7, 3.5))\n",
        "plt.bar(labels_sorted, values_sorted)\n",
        "plt.title(\"Softmax-Wahrscheinlichkeiten (Next Token)\")\n",
        "plt.ylabel(\"p\")\n",
        "plt.ylim(0, float(max(values_sorted) * 1.25))\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "for i, v in enumerate(values_sorted):\n",
        "    plt.text(i, float(v) + 0.01, f\"{float(v):.2f}\", ha=\"center\", va=\"bottom\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kompakte Tabelle (Token | Logit | e^z | Softmax)\n",
        "\n",
        "Sortiert nach Wahrscheinlichkeit (wie auf der Folie)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = pd.DataFrame({\n",
        "    \"Logit z\": [logits[t] for t in vocab],\n",
        "    \"e^z\": [exp_vals[t] for t in vocab],\n",
        "    \"Softmax p\": [probs[t] for t in vocab]\n",
        "}, index=vocab)\n",
        "\n",
        "df_final.sort_values(\"Softmax p\", ascending=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
