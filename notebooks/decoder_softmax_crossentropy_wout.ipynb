{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decoder-Vorhersage + Softmax & Cross-Entropy (Update für $W_{out}$)\n",
        "\n",
        "Dieses Notebook entspricht den Folien:\n",
        "\n",
        "## Teil 1: Decoder-Vorhersage\n",
        "Kontextvektor $h$ → Logits $z$ → Softmax $p$ → Token-ID (argmax) → Token (Tokenizer-Rückübersetzung)\n",
        "\n",
        "## Teil 2: Training-Update für $W_{out}$\n",
        "Softmax + Cross-Entropy, inkl. Gradient:\n",
        "\\[\n",
        "\\frac{\\partial L}{\\partial z} = p - y\n",
        "\\qquad\n",
        "\\frac{\\partial L}{\\partial W_{out}} = (p-y)\\,x^T\n",
        "\\]\n",
        "\n",
        "Wir reproduzieren exakt die Zahlen aus **Bsp. 1** und **Bsp. 2** auf deiner Folie.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hilfsfunktionen (Softmax, Cross-Entropy, One-Hot, Gradienten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(z: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Softmax ohne Stabilisierung, damit e^z und p wie in der Folie nachvollziehbar bleiben.\"\"\"\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e)\n",
        "\n",
        "def one_hot(k: int, n: int) -> np.ndarray:\n",
        "    y = np.zeros(n, dtype=float)\n",
        "    y[k] = 1.0\n",
        "    return y\n",
        "\n",
        "def cross_entropy(p: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"CE für One-Hot: L = -sum(y_i log p_i).\"\"\"\n",
        "    eps = 1e-12\n",
        "    return float(-np.sum(y * np.log(p + eps)))\n",
        "\n",
        "def grad_logits(p: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"dL/dz = p - y\"\"\"\n",
        "    return p - y\n",
        "\n",
        "def grad_W_out(p: np.ndarray, y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"dL/dW = (p-y) x^T\"\"\"\n",
        "    return np.outer(p - y, x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teil 1 – Decoder-Vorhersage (Logits → Softmax → Token-ID → Token)\n",
        "\n",
        "Wir verwenden ein Mini-Vokabular (so wie auf deiner Folie):\n",
        "- 12856 → \"Frankreich\"\n",
        "- 5312  → \"Deutschland\"\n",
        "- 9021  → \"Spanien\"\n",
        "\n",
        "Dann nehmen wir beispielhafte Logits und lassen das Modell per Softmax entscheiden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id_to_token = {\n",
        "    12856: \"Frankreich\",\n",
        "    5312:  \"Deutschland\",\n",
        "    9021:  \"Spanien\"\n",
        "}\n",
        "\n",
        "token_ids = list(id_to_token.keys())\n",
        "tokens = [id_to_token[i] for i in token_ids]\n",
        "\n",
        "# Beispiel-Logits (kannst du beliebig ändern)\n",
        "z = np.array([3.2, 1.7, 1.2], dtype=float)  # [Frankreich, Deutschland, Spanien]\n",
        "p = softmax(z)\n",
        "\n",
        "pred_idx = int(np.argmax(p))\n",
        "pred_token_id = token_ids[pred_idx]\n",
        "pred_token = id_to_token[pred_token_id]\n",
        "\n",
        "df_pred = pd.DataFrame({\n",
        "    \"Token-ID\": token_ids,\n",
        "    \"Token\": tokens,\n",
        "    \"Logit z\": z,\n",
        "    \"Softmax p\": p\n",
        "})\n",
        "\n",
        "display(df_pred)\n",
        "print(\"\\nVorhersage (argmax):\")\n",
        "print(\"  Token-ID:\", pred_token_id)\n",
        "print(\"  Token:\", pred_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 3.5))\n",
        "plt.bar(tokens, p)\n",
        "plt.title(\"Decoder: Softmax-Wahrscheinlichkeiten\")\n",
        "plt.ylabel(\"p\")\n",
        "plt.ylim(0, float(np.max(p) * 1.25))\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
        "for i, v in enumerate(p):\n",
        "    plt.text(i, float(v) + 0.01, f\"{float(v):.2f}\", ha=\"center\", va=\"bottom\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teil 2 – Softmax & Cross-Entropy (Update für $W_{out}$)\n",
        "\n",
        "Wir reproduzieren die Folienbeispiele:\n",
        "\n",
        "## Bsp. 1 (3 Klassen)\n",
        "- Logits: $z=[2.0, 1.0, 0.1]$\n",
        "- Softmax: $p=[0.6590, 0.2424, 0.0986]$\n",
        "- Ziel (One-Hot): $y=[1,0,0]$ (Klasse 1 = \"Frankreich\")\n",
        "- Input-Vektor (aus Attention): $x=[1.5, 0.5]$\n",
        "\n",
        "Gradienten:\n",
        "- $\\frac{\\partial L}{\\partial z} = p-y = [-0.3410, 0.2424, 0.0986]$\n",
        "- $\\frac{\\partial L}{\\partial W} = (p-y) x^T$ → Matrix wie auf der Folie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Bsp. 1 (wie Folie) ----\n",
        "z1 = np.array([2.0, 1.0, 0.1], dtype=float)\n",
        "p1 = softmax(z1)\n",
        "y1 = np.array([1.0, 0.0, 0.0], dtype=float)\n",
        "x1 = np.array([1.5, 0.5], dtype=float)\n",
        "\n",
        "dL_dz1 = grad_logits(p1, y1)\n",
        "dL_dW1 = grad_W_out(p1, y1, x1)\n",
        "L1 = cross_entropy(p1, y1)\n",
        "\n",
        "df1 = pd.DataFrame({\n",
        "    \"z\": z1,\n",
        "    \"p\": p1,\n",
        "    \"y\": y1,\n",
        "    \"p-y\": dL_dz1\n",
        "}, index=[\"Klasse 1 (Frankreich)\", \"Klasse 2 (Deutschland)\", \"Klasse 3 (Spanien)\"])\n",
        "\n",
        "print(\"Bsp. 1 – Loss L = -log(p_frankreich)\")\n",
        "print(\"p_frankreich =\", round(float(p1[0]), 4))\n",
        "print(\"L =\", round(L1, 3))\n",
        "display(df1)\n",
        "\n",
        "dfW1 = pd.DataFrame(dL_dW1, columns=[\"x1\", \"x2\"], index=[\"Klasse 1\", \"Klasse 2\", \"Klasse 3\"]) \n",
        "print(\"Gradient dL/dW = (p-y) x^T (soll Folienwerten entsprechen):\")\n",
        "display(dfW1.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check gegen Folie (Bsp. 1)\n",
        "\n",
        "Erwartete Gradientenmatrix (Folie):\n",
        "- Klasse 1: [-0.5115, -0.1705]\n",
        "- Klasse 2: [ 0.3636,  0.1212]\n",
        "- Klasse 3: [ 0.1479,  0.0493]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected_W1 = np.array([\n",
        "    [-0.5115, -0.1705],\n",
        "    [ 0.3636,  0.1212],\n",
        "    [ 0.1479,  0.0493]\n",
        "])\n",
        "\n",
        "print(\"Max. Abweichung (|Notebook - Folie|):\", float(np.max(np.abs(dL_dW1 - expected_W1))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bsp. 2 (4 Klassen)\n",
        "\n",
        "- Logits: $z=[3.52, 2.37, 1.185, 0.07]$\n",
        "- Softmax: $p=[0.692, 0.219, 0.066, 0.022]$\n",
        "- Ziel (One-Hot): $y=[1,0,0,0]$ (Klasse 1 = \"Frankreich\")\n",
        "- Input-Vektor: $x=[1.2, 0.8, 0.5]$\n",
        "\n",
        "Gradient:\n",
        "\\[\n",
        "\\frac{\\partial L}{\\partial W} = (p-y) x^T\n",
        "\\]\n",
        "soll die 4×3 Matrix aus deiner Folie ergeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Bsp. 2 (wie Folie) ----\n",
        "z2 = np.array([3.52, 2.37, 1.185, 0.07], dtype=float)\n",
        "p2 = softmax(z2)\n",
        "y2 = np.array([1.0, 0.0, 0.0, 0.0], dtype=float)\n",
        "x2 = np.array([1.2, 0.8, 0.5], dtype=float)\n",
        "\n",
        "dL_dz2 = grad_logits(p2, y2)\n",
        "dL_dW2 = grad_W_out(p2, y2, x2)\n",
        "L2 = cross_entropy(p2, y2)\n",
        "\n",
        "df2 = pd.DataFrame({\n",
        "    \"z\": z2,\n",
        "    \"p\": p2,\n",
        "    \"y\": y2,\n",
        "    \"p-y\": dL_dz2\n",
        "}, index=[\"Klasse 1 (Frankreich)\", \"Klasse 2 (Deutschland)\", \"Klasse 3 (Spanien)\", \"Klasse 4 (Italien)\"])\n",
        "\n",
        "print(\"Bsp. 2 – Loss L = -log(p_frankreich)\")\n",
        "print(\"p_frankreich =\", round(float(p2[0]), 3))\n",
        "print(\"L =\", round(L2, 3))\n",
        "display(df2)\n",
        "\n",
        "dfW2 = pd.DataFrame(dL_dW2, columns=[\"x1\", \"x2\", \"x3\"], index=[\"Klasse 1\", \"Klasse 2\", \"Klasse 3\", \"Klasse 4\"]) \n",
        "print(\"Gradient dL/dW = (p-y) x^T:\")\n",
        "display(dfW2.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check gegen Folie (Bsp. 2)\n",
        "\n",
        "Erwartete Matrix (Folie):\n",
        "- Klasse 1: [-0.3696, -0.2464, -0.154]\n",
        "- Klasse 2: [ 0.2628,  0.1752,  0.1095]\n",
        "- Klasse 3: [ 0.0792,  0.0528,  0.033]\n",
        "- Klasse 4: [ 0.0264,  0.0176,  0.011]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected_W2 = np.array([\n",
        "    [-0.3696, -0.2464, -0.1540],\n",
        "    [ 0.2628,  0.1752,  0.1095],\n",
        "    [ 0.0792,  0.0528,  0.0330],\n",
        "    [ 0.0264,  0.0176,  0.0110]\n",
        "])\n",
        "\n",
        "print(\"Max. Abweichung (|Notebook - Folie|):\", float(np.max(np.abs(dL_dW2 - expected_W2))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zusatz: Mini-Demo eines Updates (nur zur Intuition)\n",
        "\n",
        "Die Folie sagt: Wenn das Modell besser wird, steigt z. B. $p(Frankreich)$ (z. B. auf 0.85) und der Loss sinkt.\n",
        "\n",
        "Hier zeigen wir das numerisch über die Loss-Funktion (ohne ein komplettes echtes Modelltraining zu simulieren)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_old = 0.6590\n",
        "L_old = -np.log(p_old)\n",
        "\n",
        "p_new = 0.85\n",
        "L_new = -np.log(p_new)\n",
        "\n",
        "print(\"Alt:  p_frankreich =\", p_old, \"-> L =\", round(float(L_old), 3))\n",
        "print(\"Neu:  p_frankreich =\", p_new, \"-> L =\", round(float(L_new), 3))\n",
        "print(\"Loss wurde kleiner, weil -log(p) bei größerem p sinkt.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisierung: -log(p) für p in (0,1)\n",
        "ps = np.linspace(0.01, 0.999, 200)\n",
        "Ls = -np.log(ps)\n",
        "\n",
        "plt.figure(figsize=(7, 3.5))\n",
        "plt.plot(ps, Ls)\n",
        "plt.title(\"Cross-Entropy für korrektes Token: L = -log(p)\")\n",
        "plt.xlabel(\"p (Wahrscheinlichkeit für korrektes Token)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
