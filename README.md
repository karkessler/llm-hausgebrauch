# LLM f√ºr den Hausgebrauch

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.18293327.svg)](https://doi.org/10.5281/zenodo.18293327)

Technische Einf√ºhrung in Large Language Models (LLMs) f√ºr Studierende und Praktiker:innen.

## Inhalte

- Transformer-Architektur und Self-Attention
- Tokenisierung, Embeddings, Backpropagation
- Vektor-Datenbanken und RAG
- Grenzen und Risiken von LLMs

## Materialien

üìÑ **Skript:** [https://tutor.kkessler.de/LLM/llm-hausgebrauch.html](https://tutor.kkessler.de/LLM/llm-hausgebrauch.html)

## Notebooks

Die Jupyter-Notebooks k√∂nnen direkt in Google Colab ge√∂ffnet werden:

- [Mathematische Grundlagen](https://colab.research.google.com/github/karkessler/llm-hausgebrauch/blob/main/notebooks/kapitel_3_mathematische_grundlagen.ipynb)
- [Tokenisierung](https://colab.research.google.com/github/karkessler/llm-hausgebrauch/blob/main/notebooks/tokenisierung_beispiel.ipynb)
- [Attention Rechenbeispiel](https://colab.research.google.com/github/karkessler/llm-hausgebrauch/blob/main/notebooks/attention_rechenbeispiel.ipynb)
- [Qdrant Demo](https://colab.research.google.com/github/karkessler/llm-hausgebrauch/blob/main/notebooks/qdrant_demo.ipynb)

## Lizenz

CC BY 4.0
